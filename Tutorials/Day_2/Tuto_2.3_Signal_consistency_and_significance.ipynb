{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "omeA9c5XCLJJ",
        "tags": []
      },
      "source": [
        "<span style=\"float: left;padding: 1.3em\">![logo](https://github.com/gw-odw/odw/blob/main/Tutorials/logo.png?raw=1)</span>\n",
        "\n",
        "# Gravitational Wave Open Data Workshop\n",
        "\n",
        "## Tutorial 2.3: PyCBC Tutorial, Signal Consistency and Significance\n",
        "\n",
        "We will be using the [PyCBC](https://pycbc.org) library, which is used to study gravitational-wave data, find astrophysical sources due to compact binary mergers, and study their parameters. These are some of the same tools that the LIGO and Virgo collaborations use to find gravitational waves in LIGO/Virgo data\n",
        "\n",
        "In this tutorial we will walk through finding a peak in a noisy timeseries and estimating its significance given a simplified search. Some assumptions will be noted along the way. We will also make use of one of the standard signal consistency tests to help remove some non-Gaussian transient noise from the background.\n",
        "\n",
        "View this tutorial on [Google Colaboratory](https://colab.research.google.com/github/gw-odw/odw/blob/main/Tutorials/Day_2/Tuto_2.3_Signal_consistency_and_significance.ipynb) or launch [mybinder](https://mybinder.org/v2/gh/gw-odw/odw/HEAD).\n",
        "\n",
        "See [additional examples](https://pycbc.org/pycbc/latest/html/#library-examples-and-interactive-tutorials) and [documentation](https://pycbc.org/pycbc/latest/html/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A4Y62zYnv1Og"
      },
      "outputs": [],
      "source": [
        "# Those 2 lines are just to avoid some harmless warnings when importing packages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "U5rEJwqoCLJM",
        "tags": []
      },
      "source": [
        "## Installation (execute only if running on a cloud platform, like Google Colab, or if you haven't done the installation already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "3266wGrJCLJS",
        "tags": []
      },
      "source": [
        "> ⚠️ **Warning**: restart the runtime after running the cell below.\n",
        ">\n",
        "> To do so, click \"Runtime\" in the menu and choose \"Restart and run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "Collapsed": "false",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "editable": true,
        "id": "5FB3HBEhCLJO",
        "outputId": "5547eddd-8dbc-47f2-a886-1f374ee98b02",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0c22dc01fa614591830680643d1a65b1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# -- Use the following for Google Colab\n",
        "! pip install -q 'lalsuite==7.25' 'PyCBC==2.4.1'\n",
        "! pip install numpy==1.25.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "id": "dCZ7ctknCLJT",
        "tags": []
      },
      "source": [
        "### Significance of Virgo SNR peak of GW170814 ###\n",
        "\n",
        "We will estimate the significance of signal-to-noise peak observed in the Virgo instrument coincident with the large peaks observed in the LIGO-Hanford and LIGO-Livingston observatories.\n",
        "\n",
        "For this purpose we will consider a gravitational wave signal, whose existence has been confirmed based on the signals from LIGO detectors alone. This was in fact the case for the matched-filtering based analyses of GW170814, as they did not incorporate any information from the Virgo data.\n",
        "\n",
        "We ask ourselves the following question: *What is the probability that noise can produce a peak as large or larger than the largest peak observed in the Virgo data, and consistent with the lightspeed travel time between all three observatories?*\n",
        "\n",
        "This is a form of null hypothesis testing, where we compute a [p-value](https://en.wikipedia.org/wiki/P-value) to answer the question above.\n",
        "\n",
        "<!-- For the purpose of this notebook, we have added a few additional simplifying assumptions, and those will be stated as we go along. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "KTE7hteVCLJU"
      },
      "source": [
        "#### Read and Precondition Gravitational Strain Data ####\n",
        "\n",
        "In this section, we will read in a short segment of data around GW170814, and do some basic preconditioning as demonstrated in previous tutorials. We will also calculate the power spectrum of the data.\n",
        "\n",
        "Notably, here we assume that the power spectrum estimated from the data is constant over the short stretch of time, and isn't biased by our choice to center the estimate (very roughly) on the event time. We *do not* assume the data to be stationary, Gaussian, or free from non-astrophysical transient artefacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z3RSymsCLJV",
        "outputId": "701b58d8-1416-4bce-af9f-adc004297876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pycbc/types/array.py:36: UserWarning: Wswiglal-redir-stdio:\n",
            "\n",
            "SWIGLAL standard output/error redirection is enabled in IPython.\n",
            "This may lead to performance penalties. To disable locally, use:\n",
            "\n",
            "with lal.no_swig_redirect_standard_output_error():\n",
            "    ...\n",
            "\n",
            "To disable globally, use:\n",
            "\n",
            "lal.swig_redirect_standard_output_error(False)\n",
            "\n",
            "Note however that this will likely lead to error messages from\n",
            "LAL functions being either misdirected or lost when called from\n",
            "Jupyter notebooks.\n",
            "\n",
            "To suppress this warning, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
            "import lal\n",
            "\n",
            "  import lal as _lal\n"
          ]
        }
      ],
      "source": [
        "# The first import of matplotlib can take some time (especially on cloud platforms). This is normal.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pycbc.filter import resample_to_delta_t, highpass\n",
        "from pycbc.catalog import Merger\n",
        "from pycbc.psd import interpolate, inverse_spectrum_truncation\n",
        "\n",
        "m = Merger(\"GW170814\")\n",
        "\n",
        "ifos = ['H1', 'L1', 'V1']\n",
        "data = {}\n",
        "psd = {}\n",
        "\n",
        "plt.figure(figsize=[10, 5])\n",
        "\n",
        "for ifo in ifos:\n",
        "    # Read in and precondition the data\n",
        "    ts = m.strain(ifo).highpass_fir(15, 512)\n",
        "    data[ifo] = resample_to_delta_t(ts, 1.0/2048).crop(2, 2)\n",
        "\n",
        "    # Estimate the power spectral density of the data\n",
        "    # This chooses to use 2s samples in the PSD estimate.\n",
        "    # One should note that the tradeoff in segment length is that\n",
        "    # resolving narrow lines becomes more difficult.\n",
        "    p = data[ifo].psd(2)\n",
        "    p = interpolate(p, data[ifo].delta_f)\n",
        "    p = inverse_spectrum_truncation(p, int(2 * data[ifo].sample_rate), low_frequency_cutoff=15.0)\n",
        "    psd[ifo] = p\n",
        "\n",
        "    plt.plot(psd[ifo].sample_frequencies, psd[ifo], label=ifo)\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.ylim(1e-47, 1e-41)\n",
        "plt.xlim(20, 1024)\n",
        "plt.ylabel('$Strain^2 / Hz$')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "n8AQO8sbCLJY"
      },
      "source": [
        "#### Generate our template waveform and calculate the Signal-to-noise time series ####\n",
        "\n",
        "To calculate the signal-to-noise time series, we need to generate an estimate of the signal to use as a template. For this purpose we will assume the source black holes are non spinning, have equal mass, and they agree with the total mass estimate for the system as a whole. A better method would be to use the maximum likelihood estimate from an analysis of the LIGO data alone, however, this is sufficient for the purposes of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "SHSxmiY-CLJZ"
      },
      "outputs": [],
      "source": [
        "from pycbc.waveform import get_fd_waveform\n",
        "from pycbc.filter import matched_filter\n",
        "\n",
        "# Calculate the component mass of each black hole in the detector frame\n",
        "cmass = (m.median1d(\"mass1\")+m.median1d(\"mass2\")) / 2      # This is in the source frame\n",
        "cmass *= (1 + m.median1d(\"redshift\")) # apply redshift to get to the detector frame\n",
        "\n",
        "# This is a frequency domain waveform generator. It has a very similar syntax to the time domain\n",
        "# waveform function used in prior tutorials. This function returns both a plus and a cross\n",
        "# polarization waveform, but we will just use the plus polarization in building our template\n",
        "# as these are only different by a phase offset in this specific case.\n",
        "hp, _ = get_fd_waveform(approximant=\"IMRPhenomD\",\n",
        "                         mass1=cmass, mass2=cmass,\n",
        "                         f_lower=20.0, delta_f=data[ifo].delta_f)\n",
        "hp.resize(len(psd[ifo]))\n",
        "\n",
        "# For each observatory use this template to calculate the SNR time series\n",
        "snr = {}\n",
        "for ifo in ifos:\n",
        "    snr[ifo] = matched_filter(hp, data[ifo], psd=psd[ifo], low_frequency_cutoff=20)\n",
        "    snr[ifo] = snr[ifo].crop(5, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "ysNrumWSCLJe"
      },
      "outputs": [],
      "source": [
        "# Show a couple sizes\n",
        "for w, title in [(8, 'Wide View'), (.15, 'Close to GW170814')]:\n",
        "    plt.figure(figsize=[14, 4])\n",
        "    for ifo in ifos:\n",
        "        plt.plot(snr[ifo].sample_times, abs(snr[ifo]), label=ifo)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "    plt.xlim(m.time - w, m.time + w)\n",
        "    plt.ylim(0, 15)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Signal-to-noise (SNR)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "JDTw54T5CLJh"
      },
      "source": [
        "We see in the SNR time series plots above that while there are nice peaks around GW170814 in each detector, there are also some large peaks at other times. LIGO / Virgo data does contain transient (i.e limited duration) noise artefacts that an analysis must deal with to search LIGO data with high sensitivity. One approach for dealing with this is outlined later in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "yJ6CIGdACLJi"
      },
      "source": [
        "#### How well is the data actually fitting our model? ####\n",
        "\n",
        "One of the ways we can test how well the data actual fits the models to use a $\\chi^2$-based signal consistency test. We employ a version of the test described [in this paper](https://arxiv.org/pdf/gr-qc/0405045.pdf). Schematically, we chop up our template into $p$ number of frequency bins and see how much the SNR in each individual bin contributes to the total SNR ($\\rho_i$) of the supposed signal. The frequency bins are defined such that we expect that there should be a roughly equal contribution to the SNR from each. We can then calculate our statistic as the difference between the SNR in one bin, and the expected fraction of the total SNR ($\\rho$).\n",
        "\n",
        "$\n",
        "\\chi^2 = \\sum^p_{i=0} (\\rho_i - \\rho / p)^2\n",
        "$\n",
        "\n",
        "This will have $2p-2$ degrees of freedom as each SNR is *complex* representing both possible orthogonal phases the signal could have contributions from. There is also a constraint due to the fact that the sum of each bin must each add up to the total SNR by definition. In this notebook we will normalize this statistic by dividing by the number of degrees of freedom, producing $\\chi^2_r$.\n",
        "\n",
        "We expect that this statistic will be high when the template does not match well with the data, and near unity when the data either is Gaussian noise, or it contains the expected signal in addition to Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "Zk2SkSZlCLJj"
      },
      "outputs": [],
      "source": [
        "# WARNING!! If you are having problems with this code, replace the import with\n",
        "#from pycbc_chisq import power_chisq\n",
        "from pycbc.vetoes import power_chisq\n",
        "\n",
        "chisq = {}\n",
        "for ifo in ifos:\n",
        "    # The number of bins to use. In principle, this choice is arbitrary. In practice,\n",
        "    # this is empirically tuned.\n",
        "    nbins = 26\n",
        "    chisq[ifo] = power_chisq(hp, data[ifo], nbins, psd[ifo], low_frequency_cutoff=20.0)\n",
        "    chisq[ifo] = chisq[ifo].crop(5, 4)\n",
        "\n",
        "    dof = nbins * 2 - 2\n",
        "    chisq[ifo] /= dof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "Pj4cS9DkCLJn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[14, 4])\n",
        "\n",
        "for ifo in ifos:\n",
        "    plt.plot(chisq[ifo].sample_times, chisq[ifo], label=ifo)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim(m.time -0.15, m.time + 0.15)\n",
        "plt.ylim(0, 5)\n",
        "plt.ylabel('$chi^2_r$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "GZhZPz5LCLJq"
      },
      "source": [
        "There are some notable features in the $\\chi^2_r$ time series. We see that there is a dip in the value at the time of the peak in the SNR in each observatory. We expect this as the template now aligns with the signal in the data. Also, the values climb just around this minima. This occurs because the template is starting to slide against the true signal in the data but is not perfectly aligned with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "mD4UI9ALCLJq"
      },
      "source": [
        "#### Re-weighting our SNR to help down-weight times that don't fit our signal ####\n",
        "\n",
        "One approach we can take to improve the sensitivity of the search is to down-weight the times where the data does not appear as either Gaussian noise or Gaussian noise + our template. We can do this be combining the SNR time series and our $\\chi^2_r$ time series. This is a method used to re-weight the SNR calculated by the PyCBC pipeline since initial LIGO, and has been employed in the first three Advanced LIGO observing runs. In this tutorial we will choose to use the [re-weighted SNR $\\hat{\\rho}$](https://pycbc.org/pycbc/latest/html/_modules/pycbc/events/ranking.html) ([described in this paper originally](https://iopscience.iop.org/article/10.1088/0264-9381/33/21/215004/pdf), with [more discussion in this work](https://iopscience.iop.org/article/10.1088/1361-6382/ab685e)) to rank our events:\n",
        "\n",
        "$\\hat{\\rho} = \\frac{\\rho}{ \\frac{1}{2}[1 + (\\chi^2_r)^3]^{1/6}}$ where $\\chi^2 > 1$, otherwise $\\rho$\n",
        "\n",
        "For reference on how we rank coincident (i.e. occurring in multiple detector) events in Advanced LIGO, there is a description [here](https://iopscience.iop.org/article/10.3847/1538-4357/aa8f50/pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "MKJV5tv1CLJs"
      },
      "outputs": [],
      "source": [
        "from pycbc.events.ranking import newsnr\n",
        "\n",
        "# The rho-hat term above is named \"newsnr\" here\n",
        "nsnr = {ifo:newsnr(abs(snr[ifo]), chisq[ifo]) for ifo in ifos}\n",
        "\n",
        "# Show a couple sizes\n",
        "for w, title in [(8, 'Wide View'), (.15, 'Close to GW170814')]:\n",
        "    plt.figure(figsize=[14, 4])\n",
        "    for ifo in ifos:\n",
        "        plt.plot(snr[ifo].sample_times, nsnr[ifo], label=ifo)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "    plt.xlim(m.time - w, m.time + w)\n",
        "    plt.ylim(0, 15)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Re-weighted Signal-to-noise')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "EYp1ZppWCLJw"
      },
      "source": [
        "We can see above that there are still peaks around GW170814 in all detectors at roughly the same signal strength. At other times - where there were previously peaks in the time series due to transient noise - there are no longer large SNR values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "6v4oj4T1CLJx"
      },
      "source": [
        "#### Calculating the background and significance ####\n",
        "\n",
        "In this section we will determine how significant the peak in the Virgo re-weighted SNR time series is.\n",
        "\n",
        "We will do this first by determining where one might expect a peak associated with an astrophysical source relative to the LIGO observed peaks. This is set by the constraint that an astrophysical source can only cause delays between observatories no larger than the light travel time between them. The [`pycbc.detector.Detector`](https://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.detector.Detector) class provides some convenient methods to calculate the light travel time between detectors.\n",
        "\n",
        "We will then identify the largest peak in the SNR for this window around the LIGO observed peaks. This is our \"on-source\" window.\n",
        "\n",
        "Finally, to determine the significance of the peak detected in the on-source window, we will compare how likely it is for a peak as large or larger to appear in the background. Our background will be empirically measured by taking portions of the SNR time series from an \"off-source\" window i.e. times where it is not possible for the signal to travel from one detector to another within the light travel time.\n",
        "\n",
        "An important criteria to avoid a biased significance estimate is that algorithm to identify the background (off-source) and foreground (on-source) peaks is the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "hjIUBtnuCLJx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from pycbc.detector import Detector\n",
        "\n",
        "# Calculate the time of flight between the Virgo detectors and each LIGO observatory\n",
        "d = Detector(\"V1\")\n",
        "tof = {}\n",
        "tof['H1'] = d.light_travel_time_to_detector(Detector(\"H1\"))\n",
        "tof['L1'] = d.light_travel_time_to_detector(Detector(\"L1\"))\n",
        "\n",
        "# Record the time of the peak in the LIGO observatories\n",
        "ptime = {}\n",
        "\n",
        "plt.figure(figsize=[14, 4])\n",
        "for ifo in ifos:\n",
        "\n",
        "    # shade the region around each LIGO peak that could have a peak in Virgo if from\n",
        "    # an astrophysical source\n",
        "    if ifo != 'V1':\n",
        "        ptime[ifo] = snr[ifo].sample_times[nsnr[ifo].argmax()]\n",
        "        plt.axvspan(ptime[ifo] - tof[ifo], ptime[ifo] + tof[ifo], alpha=0.2, lw=10)\n",
        "\n",
        "    plt.plot(snr[ifo].sample_times, nsnr[ifo], label=ifo)\n",
        "\n",
        "# Calculate the span of time that a Virgo peak could in principle happen in from time of flight\n",
        "# considerations.\n",
        "start = ptime['H1'] - tof['H1']\n",
        "end = ptime['L1'] + tof['L1']\n",
        "\n",
        "# convert the times to indices along with how large the region is in number of samples\n",
        "window_size = int((end - start) * snr['V1'].sample_rate)\n",
        "sidx = int((start - snr['V1'].start_time) * snr['V1'].sample_rate)\n",
        "eidx = sidx + window_size\n",
        "\n",
        "# Calculate the \"on-source\" peak re-weighted (newsnr) statistic value.\n",
        "onsource = nsnr['V1'][sidx:eidx].max()\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim(m.time - .08, m.time + .08)\n",
        "plt.ylim(0, 15)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Re-weighted Signal-to-noise')\n",
        "plt.show()\n",
        "\n",
        "print('Virgo Peak has a re-weighted SNR value of {}'.format(onsource))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "DHLZIDeFCLJ0"
      },
      "source": [
        "In the plot above we see the re-weighted SNR time series. On top of that we've shaded the regions which are consistent with a Virgo signal based on the peaks in the LIGO observatories. Only in the darker region, is it possible to have a peak in the SNR that is consistent with both LIGO observatories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "D9_fJTDvCLJ0"
      },
      "outputs": [],
      "source": [
        "# Now that we've calculated the onsource peak, we should calculate the background peak values.\n",
        "# We do this by chopping up the time series into chunks that are the same size as our\n",
        "# on-source window and repeating the same peak finding (max) procedure - keeping the algorithm\n",
        "# the same to prevent bias\n",
        "\n",
        "# Walk through the data in chunks and calculate the peak statistic value in each.\n",
        "peaks = []\n",
        "i = 0\n",
        "while i + window_size < len(nsnr['V1']):\n",
        "    p = nsnr['V1'][i:i+window_size].max()\n",
        "    peaks.append(p)\n",
        "    i += window_size\n",
        "\n",
        "    # Skip past the onsource time\n",
        "    if abs(i - sidx) < window_size:\n",
        "        i += window_size * 2\n",
        "\n",
        "peaks = numpy.array(peaks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "id": "Qiym0pdVCLJ3"
      },
      "outputs": [],
      "source": [
        "# The p-value is just the number of samples observed in the background with a\n",
        "# value equal or higher than the onsource divided by the number of samples.\n",
        "# We can make the mapping between statistic value and p-value using our background\n",
        "# samples.\n",
        "pcurve = numpy.arange(1, len(peaks)+1)[::-1] / float(len(peaks))\n",
        "peaks.sort()\n",
        "\n",
        "pvalue = (peaks > onsource).sum() / float(len(peaks))\n",
        "\n",
        "plt.figure(figsize=[10, 7])\n",
        "plt.scatter(peaks, pcurve, label='Off-source (Noise Background)', color='black')\n",
        "\n",
        "plt.axvline(onsource, label='On-source', color='red')\n",
        "plt.axhline(pvalue, color='red')\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid()\n",
        "plt.ylim(1e-3, 1e0)\n",
        "plt.ylabel('p-value')\n",
        "plt.xlabel('Re-weighted Signal-to-noise')\n",
        "\n",
        "plt.xlim(2, 5)\n",
        "plt.show()\n",
        "\n",
        "print(\"The p-value associated with the GW170814 peak is {}\".format(pvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "f9w4ucRbCLJ5"
      },
      "source": [
        "In this tutorial, we find a peak in Virgo as large as the obseved one has an approximately 2% chance of occurring due to the noise alone. Since that is a relatively low probability, we may reject the null hypothesis that the observed peak is due to noise alone. Given the simplifications of this tutorial, we find a result in agreement with the [GW170814 discovery paper](https://arxiv.org/pdf/1709.09660.pdf) which reported a p-value of 0.3%. The reason for the slight discrepancy in these numbers is due to the duration over which the background is collected.\n",
        "\n",
        "If the signal was much louder in the Virgo data, the Virgo peak would be larger than any peak in the noise background. In this case, this method of estimating the significance would only be able to set an upper bound on the p-value of the observed peak. In order to calculate the p-value of a much larger peak, we would either need to use more background data or make additional assumptions about the background distribution. If a gravitational-wave signal is extremely loud, it is challenging to calculate the precise significance of the observed peak, but we can still be confident that the signal is very significant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "LnMJRzdfv1Oj"
      },
      "source": [
        "## Challenge!\n",
        "\n",
        "Use the methods demonstrated above to see if you can calculate the SNR time series and re-weighted SNR timeseries in the following data set. This data set contains one signal and two glitches. At what times do you find peaks in the SNR timeseries? Which peaks are still present in the re-weighted SNR timeseries?\n",
        "\n",
        "Information that may be useful:\n",
        "\n",
        "* The signal and glitches are all placed between 100 and 120 seconds into the frame file.\n",
        "* You may assume mass1 = mass2 (equal mass) and that the component mass of the signal is 32.\n",
        "* Each file starts at gps time 0, and ends at gps time 128\n",
        "* The channel name in each file is \"H1:TEST-STRAIN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "tags": [],
        "id": "1_fjx59Gv1Oj"
      },
      "outputs": [],
      "source": [
        "# Download the challenge set files\n",
        "from pycbc.frame import read_frame\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "def get_file(fname):\n",
        "    url = \"https://github.com/gw-odw/odw/raw/main/Tutorials/Day_2/{}\"\n",
        "    url = url.format(fname)\n",
        "    urllib.request.urlretrieve(url, fname)\n",
        "    print('Getting : {}'.format(url))\n",
        "\n",
        "files = ['Data/PyCBC_T3_0.gwf']\n",
        "\n",
        "# Download the file if needed\n",
        "for fname in files:\n",
        "    if not os.path.exists(fname):\n",
        "        os.makedirs(\"Data\", exist_ok=True)\n",
        "        get_file(fname)\n",
        "\n",
        "\n",
        "# An example of how to read the data from these files:\n",
        "file_name = \"Data/PyCBC_T3_0.gwf\"\n",
        "\n",
        "# LOSC bulk data typically uses the same convention for internal channels names\n",
        "# Strain is typically IFO:LOSC-STRAIN, where IFO can be H1/L1/V1.\n",
        "channel_name = \"H1:TEST-STRAIN\"\n",
        "\n",
        "start = 0\n",
        "end = start + 128\n",
        "\n",
        "ts = read_frame(file_name, channel_name, start, end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "editable": true,
        "tags": [],
        "id": "L4yoN2o5v1Oj"
      },
      "outputs": [],
      "source": [
        "import pylab\n",
        "def challenge_matched_filter(file_name,mass):\n",
        "  print(\"Looking at file {} with template mass {} M_sol\".format(file_name,mass))\n",
        "  channel_name = \"H1:TEST-STRAIN\"\n",
        "  start = 0\n",
        "  end = start + 128\n",
        "  ts = read_frame(file_name, channel_name, start, end)\n",
        "  ts = highpass(ts, 15.0)\n",
        "  strain_ts = resample_to_delta_t(ts, 1.0/2048)\n",
        "  conditioned_ts = strain_ts.crop(2, 2)\n",
        "  psd_ts = conditioned_ts.psd(4)\n",
        "  psd_ts = interpolate(psd_ts, conditioned_ts.delta_f)\n",
        "  psd_ts = inverse_spectrum_truncation(psd_ts, int(4 * conditioned_ts.sample_rate),\n",
        "  low_frequency_cutoff=15)\n",
        "  hp_x, _ = get_fd_waveform(approximant=\"IMRPhenomD\",\n",
        "     mass1=mass, mass2=mass,\n",
        "     f_lower=20.0, delta_f=conditioned_ts.delta_f)\n",
        "  hp_x.resize(len(psd_ts))\n",
        "\n",
        "  # For each observatory use this template to calculate the SNR time series\n",
        "  snr_x = matched_filter(hp_x, conditioned_ts, psd=psd_ts, low_frequency_cutoff=20).crop(5, 4)\n",
        "\n",
        "  pylab.figure(figsize=[14, 4])\n",
        "  pylab.plot(snr_x.sample_times, abs(snr_x), label='H1')\n",
        "  pylab.title('SNR Time Series')\n",
        "  pylab.grid()\n",
        "  pylab.xlim(100,120)\n",
        "  pylab.ylim(0, 15)\n",
        "  pylab.xlabel('Time (s)')\n",
        "  pylab.ylabel('Signal-to-noise (SNR)')\n",
        "  pylab.show()\n",
        "\n",
        "  chisq = {}\n",
        "  nbins = 26\n",
        "  chisq_x = power_chisq(hp_x, conditioned_ts, nbins, psd_ts, low_frequency_cutoff=20.0)\n",
        "  chisq_x = chisq_x.crop(5, 4)\n",
        "\n",
        "  dof_x = nbins * 2 - 2\n",
        "  chisq_x /= dof_x\n",
        "\n",
        "\n",
        "  # The rho-hat term above is named \"newsnr\" here\n",
        "  nsnr_x = newsnr(abs(snr_x), chisq_x)\n",
        "\n",
        "  # Plot the new SNR timeseries\n",
        "  pylab.figure(figsize=[14, 4])\n",
        "  pylab.plot(snr_x.sample_times, nsnr_x, label='H1')\n",
        "  pylab.title('NewSNR Timeseries')\n",
        "  pylab.grid()\n",
        "  pylab.xlim(100,120)\n",
        "  pylab.ylim(0, 15)\n",
        "  pylab.xlabel('Time (s)')\n",
        "  pylab.ylabel('Re-weighted Signal-to-noise')\n",
        "  pylab.show()\n",
        "\n",
        "challenge_matched_filter(files[0], 32)"
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "colab": {
      "name": "Tuto_2.3_Signal_consistency_and_significance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of contents:",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}